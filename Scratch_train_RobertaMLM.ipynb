{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e857ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --ignore-installed PyYAML\n",
    "#!pip install transformers\n",
    "#!pip install tensorflow\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0708135-84f0-4724-b4f7-db6f97cf5b89",
   "metadata": {},
   "source": [
    "# Train a model from scratch (RobertaForMaskedLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97edbc-7c94-413e-bfba-b07c6d69638c",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a254cc-1380-43e2-a475-3438c5315276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset oscar (/home/xiaopengxu/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_it/1.0.0/e4f06cecc7ae02f7adf85640b4019bf476d44453f251a1d84aebae28b0f8d51d)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': \"La estrazione numero 48 del 10 e LOTTO ogni 5 minuti e' avvenuta sabato 15 settembre 2018 alle ore 04:00 a Roma, nel Centro Elaborazione Dati della Lottomatica Italia (ora GTech SpA), con la supervisione della Amministrazione Autonoma dei Monopoli di Stato (AAMS), incaricata di vigilare sulla regolarità delle operazioni di sorteggio.\\nIl Montepremi della 48ª estrazione viene ripartito tra i vincitori delle singole categorie di premio.\\nRicorda di controllare il Numero ORO 53. E, se lo hai giocato, anche il DOPPIO ORO 53 e 66. Se indovini puoi vincere premi più ricchi.\\nIl nostro sito web impiega cookies per migliorare la navigazione del visitatore. L’utente è consapevole che, continuando a visitare il nostro sito web, accetta l’utilizzo dei cookies Accetto Informazioni\\n(C) Copyright 2013-2017 10elotto.biz | Il presente sito è da considerarsi un sito indipendente, NON collegato alla rete ufficiale Gtech SpA.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_it')\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dd3ba41-6c41-4882-a69b-3a7a511d66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls -alh *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6118368-a92d-4d2a-b842-8b5d6aafa4c1",
   "metadata": {},
   "source": [
    "### Save data to train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "083a6016-0179-4c37-94a5-7fd570fde4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9740d321f34ff4adfb81a4ff345c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28522082.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "    sample = sample['text'].replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 10_000:\n",
    "        # once we git the 10K mark, save to file\n",
    "        with open(f'./oscar_it/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too\n",
    "with open(f'./oscar_it/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55bd373-df16-4802-b2c1-a55d6f34da0c",
   "metadata": {},
   "source": [
    "## Building a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75ac0408-4533-4923-8e33-c2be4669f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('./oscar_it').glob('**/*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dca6058-b243-4fd6-8919-56f27b290931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7627c9ba-b10d-4a44-9591-befb9c58f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=paths[:5], vocab_size=30_522, min_frequency=2,\n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f7ecf-51e0-486f-b75b-4c6d7b7804c3",
   "metadata": {},
   "source": [
    "### Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70883c66-e01e-462d-99c1-a3decfb85fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filiberto/vocab.json', 'filiberto/merges.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir('./filiberto')\n",
    "\n",
    "tokenizer.save_model('filiberto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6d62b-7c5c-4f30-a2d3-c919fd96bb81",
   "metadata": {},
   "source": [
    "### Initializing the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cbd348f-93f5-4110-9c4c-aa37427ea821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# initialize the tokenizer using the tokenizer we initialized and saved to file\n",
    "tokenizer = RobertaTokenizer.from_pretrained('filiberto', max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c551cac4-82e3-45a0-84e6-cb9de597b441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 13722, 16, 479, 600, 35, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# test our tokenizer on a simple sentence\n",
    "tokens = tokenizer('ciao, come va?')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649387ef-466d-4052-92e8-5fca3c8acc88",
   "metadata": {},
   "source": [
    "## Creating the Input Pipeline\n",
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8913fc2a-0048-42ae-b48b-6514d7b931fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./oscar_it/text_0.txt', 'r', encoding='utf-8') as fp:\n",
    "    lines = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6712e64b-f4b2-4055-963b-dfcab2303796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = tokenizer(lines, max_length=512, padding='max_length', truncation=True)\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7463797-4d59-4837-96ae-19f05c5bea1a",
   "metadata": {},
   "source": [
    "### Building the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df6e3323-c258-42ed-a572-0be1fb77cada",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5e64f86499cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e44143-a6f0-44e2-ba3f-45b7fefedc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f5cec-dc2c-4f25-8371-29ef949e1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a699701-1435-456c-9fac-5c249981a170",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e644a17-9ca5-4112-969b-a4811105874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=30_522,  # we align this to the tokenizer vocab_size\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636998fb-15c2-4088-b58c-b61b20ea4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00499978-5b7a-4ff6-a171-ec8bbae8c7cd",
   "metadata": {},
   "source": [
    "### Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ed070-13d0-4ebf-966a-851ee99d78e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911837f4-5f8e-4e4b-9cdf-981bdb39bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac939cb-3a42-4be3-b8e0-c00dc082f2e2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441eb64-6175-4277-aff1-3adb146647ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acdc81b-fafd-4f3d-b6db-63d045e7f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./filiberto')  # and don't forget to save filiBERTo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e07b5d-8286-4e93-92e6-0393dac90a0d",
   "metadata": {},
   "source": [
    "## The Real Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d78407-4aaf-4e53-a07b-a6173c3f7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill = pipeline('fill-mask', model='filiberto', tokenizer='filiberto')\n",
    "fill(f'ciao {fill.tokenizer.mask_token} va?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6098ae8-2abc-49f7-955d-f4513e92570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill(f'buongiorno, {fill.tokenizer.mask_token} va?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3e06c-b0a4-4d2f-8b64-1ebf2894d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill(f'ciao, dove ci {fill.tokenizer.mask_token} oggi pomeriggio? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83981823-0cdb-4c91-bb5b-48512a231d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill(f'cosa sarebbe successo se {fill.tokenizer.mask_token} scelto un altro giorno?') # a harder sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d087f-dcc2-4588-aa2d-c825bfa822d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
