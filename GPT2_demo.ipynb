{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0345a971-d558-4d80-b76b-ea9e0bbfd80b",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a169f15e-c6c2-4e34-9ad4-3ea3072f2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip uninstall numpy\n",
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9942b8c5-97be-495e-8fa3-3dac4d678a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 10:05:10.374620: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from gensim.corpora import WikiCorpus\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "file_path = './wiki-latest-pages-articles.xml.bz2'\n",
    "lang = 'en'\n",
    "\n",
    "def store(corpus, lang):\n",
    "    base_path = os.getcwd()\n",
    "    store_path = os.path.join(base_path, '{}_corpus'.format(lang))\n",
    "    if not os.path.exists(store_path):\n",
    "        os.mkdir(store_path)\n",
    "    file_idx=1\n",
    "    for text in corpus.get_texts():\n",
    "        current_file_path = os.path.join(store_path, 'article_{}.txt'.format(file_idx))\n",
    "        with open(current_file_path, 'w' , encoding='utf-8') as file:\n",
    "            file.write(bytes(' '.join(text), 'utf-8').decode('utf-8'))\n",
    "        #endwith\n",
    "        file_idx += 1\n",
    "    #endfor\n",
    "\n",
    "def tokenizer_func(text: str, token_min_len: int, token_max_len: int, lower: bool) -> list:\n",
    "    return [token for token in text.split() if token_min_len <= len(token) <= token_max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a03837f-7390-4752-a5b9-dfcdb4dd78dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41937/374683988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikiCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = WikiCorpus(file_path, lower=False, tokenizer_func=tokenizer_func)\n",
    "lang = 'en'\n",
    "store(corpus, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad80d4-39d0-4473-b144-2306771ba0e3",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8bd11-e75b-47c0-b3e4-a857a74d1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "class BPE_token(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        self.tokenizer.normalizer = Sequence([NFKC()])\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    def bpe_train(self, paths):\n",
    "        trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\"\n",
    "        ])\n",
    "        self.tokenizer.train(trainer, paths)\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d054e-78ac-4102-9b3f-f9b718c0623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tokenise import BPE_token\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# the folder 'text' contains all the files\n",
    "paths = [str(x) for x in Path(\"./text/\").glob(\"**/*.txt\")]\n",
    "tokenizer = BPE_token()\n",
    "# train the tokenizer model\n",
    "tokenizer.bpe_train(paths)\n",
    "# saving the tokenized data in our specified folder \n",
    "save_path = 'tokenized_data'\n",
    "tokenizer.save_tokenizer(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97ab04-851c-4384-87f0-26bacb8c7d10",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde9e14-60bf-4d90-98d0-996824e0c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# loading tokenizer from the saved model path\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})\n",
    "# creating the configurations from which the model can be made\n",
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer.vocab_size,\n",
    "  bos_token_id=tokenizer.bos_token_id,\n",
    "  eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# creating the model\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6f4da-d403-448d-b1b3-7d31f39bfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_string = ''\n",
    "\n",
    "for filename in paths:\n",
    "  with open(filename, \"r\", encoding='utf-8') as f:\n",
    "   x = f.read()\n",
    "  single_string += x + tokenizer.eos_token\n",
    "\n",
    "string_tokenized = tokenizer.encode(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d278cf-2c1a-4692-8b8c-2806ef912d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "block_size = 100\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "  examples.append(string_tokenized[i:i + block_size])\n",
    "inputs, labels = [], []\n",
    "for ex in examples:\n",
    "  inputs.append(ex[:-1])\n",
    "  labels.append(ex[1:])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e873d-1fb0-4989-9024-395fdd7b0677",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4b88e-3476-4699-bc84-47eaee9430a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# definining our loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# defining our metric which we want to observe\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# compiling the model\n",
    "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca8e05-2d1d-4591-8dc0-d049281e0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 10\n",
    "history = model.fit(dataset, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c56446-b6f7-4094-b66c-393e43a0eb4f",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b34ec-1828-4b35-80f5-48cdad8c0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"লালমোহনবাবু \"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "beam_output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 50,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.7,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bca573-f24d-402a-99ca-24d1eb4d2de3",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088ba0b-74db-4835-899b-0ad30b396a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "import os\n",
    "\n",
    "output_dir = './model_bn_custom/'\n",
    "# creating directory if it is not present\n",
    "if not os.path.exists(output_dir):\n",
    "  os.mkdir(output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "# save model and model configs\n",
    "model.save_pretrained(output_dir)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "# save tokenizer\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00dc8a-cbef-430e-9e15-73bdd350a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usage\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "model = TFGPT2LMHeadModel.from_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
